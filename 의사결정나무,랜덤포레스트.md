## 의사결정나무

---

데이터를 분석하여 이들 사이에 존재하는 패턴을 예측 가능한 규칙들의 조합으로 나타내며 그 모양이 나무와 같다고 해서 의사결정나무라 불린다. 

루트 마디, 중간 마디, 잎 마디. 초기지점은 루트노드이고 분기가 거듭될 수록 그에 해당하는 데이터의 개수는 줄어든다. 각 잎마디에 속하는 데이터의 개수를 합하면 루트 노드의 데이터수와 일치한다. 잎 노드에 속하는 데이터의 개수를 합하면 루트 노드의 데이터수와 일치한다. 잎 노드간 교집합이 없다.

의사결정나무는 분류와 회귀 모두 가능하다. 범주나 연속형 수치 모두 예측할 수 있다. 분류과정은 새로운 데이터가 특정 잎 노드에 속한다는 정보를 확인한 뒤 해당 잎 노드에서 가장 빈도가 높은 범주에 새로운 데이터를 분류하게 된다. 

의사결정나무는 한번 분기 때마다 변수 영역을 두개로 구분하는 모델이다. 의사결정나무는 구분 뒤 각 영역의 순도가 증가, 불순도 혹은 불확실성이 최대한 감소하도록 하는 방향으로 학습을 진행한다. 순도가 증가/불확식성이 감소하는걸 두고 정보이론에서는 정보획득이라고 한다. 의사결정나무는 구분 뒤 각 영역의 순도가 증가/ 불확실성이 최대한 감소하는 방향으로 학습을 진행한다. 불순도 지표로 엔트로피와 많이 쓰이는게 바로 지니계수이다. 

의사결정나무의 학습과정은 입력 변수영역을 두 개로 구분하는 재귀적 분기와 너무 자세하게 구분된 영역을 통합하는 가지치기 두가지 과정으로 나뉜다. 

가지치기 - 모든 잎 노드의 순도가 100% 인상태를 Full tree라고 한다. Full tree를 생성한 뒤 적절한 수준에서 잎 노드를 결합해주어야 한다. 왜냐면 분기가 너무 많아서 학습데이터에 오버피팅할 염려가 생기기 때문이다. 의사결정나무의 분기 수가 증가할 때 처음에는 새로운 데이터에 대한 오분류율이 감소하나 일정 수준 이상이 되면 오분류율이 되레 증가하는 현상이 발생한다. 이러한 문제를 해결하기 위해서는 검증데이터에 대한 오뷴류율이 증가하는 시점에서 적절히 가지치기를 수행해줘야 한다.  의사결정나무는 계산복잡성 대비 높은 예측 성능을 내는 것으로 정평이 나 있다. 아울러 변수 단위로 설명력을 지닌다는 강점을 가지고 있다. 다만 의사결정나무는 **결정경계(decision boundary)**가 데이터 축에 수직이어서 특정 데이터에만 잘 작동할 가능성이 높다.

* 의사결정은 특정 데이터에서만 잘 작동할 가능성이 높아서 이를 극복하기 위하여 나온 모델이 바로 랜덤 포레스트이다!!

## 랜덤 포레스트

---

같은 데이터에 의사결정 나무 여러개를 동시에 적용해서 학습 성능을 높이는 앙상블 기법. 로테이션 포레스트는 학습 데이터에 PCA를 적용해 데이터 축을 회전 한 후 학습한다는 점을 제외하고 랜덤 포레스트와 같다.

1. 투표 기반 분류기

* 여러개의 머신러닝 모델을 학습시킨 후 , 예측값을 투표해서 최정의 클래스로 예측한다.

2. 배깅 , 페이스팅

* 하나의 알고리즘을 사용하지만, 학습 데이터셋을 랜덤하게 추출하여 모델을 각각 다르게 학습시키는 방법, 학습 데이터셋에서 랜덤하게 추출할 때, 중복을 허용하는 방식이 배깅이다. 중복을 허용하지 않으면 페이스팅!

3. 랜덤 포레스트

* 배깅을 적용한 의사결정나무
  * Etc) 엑스트라 트리, 익스트림 랜덤 트리

4. 부스팅

* 성능이 약한 학습기를 여러개 연결 -> 강한 학습기를 만드는 앙상블이다.

5. 스태킹

* 앙상블 학습에서 모델의 예측값을 가지고 새로운 메타 모델을 학습시켜 최종 예측모델을 만드는 방법이다. 